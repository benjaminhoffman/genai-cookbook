{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326936b2-05db-465d-846c-b07f2ace8000",
   "metadata": {},
   "source": [
    "Meta AI released a series of [Code Llama models](https://ai.meta.com/blog/code-llama-large-language-model-coding/). \n",
    "\n",
    "Using any of the model series has the following prompt generation flow.\n",
    "\n",
    "<img src=\"images/meta_code_llama.png\" height=\"30%\" width=\"60%\">\n",
    "\n",
    "This notebook shows some cases how to prompt the LLM to generate \n",
    "Python and SQL code using the Anyscale Endpoints.\n",
    "\n",
    "<img src=\"images/codellam-70b.png\"  height=\"25%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7920f30d-45d3-4ed6-a11b-745973d22fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16843632-a590-4624-850f-bf619dd2122a",
   "metadata": {},
   "source": [
    "Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd1b858-df9e-45c2-a502-1ef501b4de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=codellama/CodeLlama-70b-Instruct-hf; base=https://api.endpoints.anyscale.com/v1\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "print(f\"Using MODEL={MODEL}; base={openai.api_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df4a141-61d2-4729-9cba-e7cc1954184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OpenAI client, which can be used transparently with Anyscale \n",
    "# Endpoints too\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = openai.api_key,\n",
    "    base_url = openai.api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b210270-a724-4546-900d-f85bb1e3cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to send and fetch response\n",
    "\n",
    "def get_commpletion(clnt: object, model: str, system_content: str, user_content:str) -> str:\n",
    "    chat_completion = clnt.chat.completions.create(\n",
    "        model=model,\n",
    "    messages=[{\"role\": \"system\", \"content\": system_content},\n",
    "              {\"role\": \"user\", \"content\": user_content}],\n",
    "    temperature = 0.8)\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fe78f3-6250-4c39-afeb-cbbcdaba7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_BEGIN = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602d78e-e70c-4f67-9f6a-73d3551de252",
   "metadata": {},
   "source": [
    "#### Example 1: Generate a Python code to compute the value of PI\n",
    "\n",
    "Use the [CO-STAR](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) framework for prompting\n",
    "\n",
    "1. **Context** - provide the background\n",
    "2. **Objective** (or Task) - define the task to be performed\n",
    "3. **Style** - instruct a writing style. Kind of sentences; formal, informal, magazine sytle, colloqiual, or allude to a know style.\n",
    "4. **Audience** - who's it for?\n",
    "5. **Response** - format, Text, Python, SQL, JSON, etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d38fa3-9405-43d3-89df-8e334dee6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_content = \"\"\"You are supreme repository of knowledge, and assistant\n",
    "code Co-pilot for developer to assist them in generating sample code, inclding\n",
    "in langauges such as Python, JavaScript, Java, C, C++, and shell script.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2461dd5c-6afd-4635-b710-e7cee97f2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# CONTEXT #\n",
    "I want to generate Python code to showcase our product feature for\n",
    "serving open source large language models such as Code Llama series\n",
    "on Anyscale Endpoints. The product feature is Anyscale Endpoints, which \n",
    "serves all Llama series models and the Mistral series too. For this\n",
    "instance, we want to show case Code Llama 70B just released by Meta AI, and \n",
    "now hosted by Anyscale Endpoints\n",
    "\n",
    "#############\n",
    "\n",
    "# OBJECTIVE # \n",
    "Create a Python code to compute the value of PI using monte carlo method.\n",
    "\n",
    "#############\n",
    "\n",
    "# STYLE #\n",
    "Use the Google style of formating Python code.\n",
    "\n",
    "#############\n",
    "\n",
    "# AUDIENCE #\n",
    "Your code should be well commented and aimed at both beginner and\n",
    "intermediate Python programers. \n",
    "\n",
    "#############\n",
    "\n",
    "# RESPONSE #\n",
    "Generate the entire code that can be easily copy and pasted into file: 'compute_pi.py'. Also,\n",
    "provide specific instructions how to compile the code, run it, and any \n",
    "additional python packages it needs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d730cfe-544c-4947-b767-0b3b2924eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAnswer - Python code\u001b[0m:\n",
      " \n",
      "```\n",
      "#!/usr/bin/env python\n",
      "\n",
      "# import necessary packages\n",
      "import numpy as np\n",
      "import time\n",
      "from multiprocessing import Process, Pipe\n",
      "\n",
      "# Establishes the number of points for the simulation.\n",
      "num_points = 10000000\n",
      "\n",
      "# Creates a function to calculate the number of points within \n",
      "# the unit circle.\n",
      "def calculate_points(num_points):\n",
      "    points = np.random.uniform(-1, 1, (num_points, 2))\n",
      "    distances = np.linalg.norm(points, axis=1)\n",
      "    return np.sum(distances <= 1)\n",
      "\n",
      "# Creates a function to calculate the value of pi using the \n",
      "# number of points in the unit circle.\n",
      "def calculate_pi(num_points):\n",
      "    return 4 * calculate_points(num_points) / num_points\n",
      "\n",
      "# A function to run this computation in parallel. \n",
      "# It uses multiprocessing and Pipe to connect the processes and send\n",
      "# the resulting value of pi through the pipe.\n",
      "def run_in_parallel(num_points):\n",
      "    pipe1, pipe2 = Pipe()\n",
      "    p = Process(target=calculate_pi, args=(num_points, pipe1))\n",
      "    p.start()\n",
      "    result = pipe2.recv()\n",
      "    p.join()\n",
      "    return result\n",
      "\n",
      "# Creates and returns the value of PI using the parallel method.\n",
      "def compute_pi(num_points):\n",
      "    start = time.time()\n",
      "    pi_value = run_in_parallel(num_points)\n",
      "    end = time.time()\n",
      "    print(\"Time taken: \", end - start)\n",
      "    print(\"Value of PI: \", pi_value)\n",
      "    return pi_value\n",
      "\n",
      "# Calls the function to compute the value of PI.\n",
      "compute_pi(num_points)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "### Compile Instructions:\n",
      "\n",
      "To run this code, you will need to install the necessary Python packages, which are:\n",
      "\n",
      "1. numpy\n",
      "2. multiprocessing\n",
      "\n",
      "You can install these packages using pip:\n",
      "\n",
      "```\n",
      "pip install numpy\n",
      "pip install multiprocessing\n",
      "```\n",
      "\n",
      "Once you have installed the packages, open a command prompt or terminal and navigate to the directory where you have saved the compute_pi.py file. Then, run the following command:\n",
      "\n",
      "```\n",
      "python compute_pi.py\n",
      "```\n",
      "\n",
      "This will start the simulation and calculate the value of PI. The result will be printed to your console.\n",
      "\n",
      "I hope this helps! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = get_commpletion(client, MODEL, system_content, user_prompt)\n",
    "print(f\"\\n{BOLD_BEGIN}Answer - Python code{BOLD_END}:\\n {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4de9f-462e-43ee-bbd0-1c1209d15dfe",
   "metadata": {},
   "source": [
    "### Example 2: Generate a Python code generate fractions\n",
    "\n",
    "Generate Python code that a) generates fractions between 1 and 10, finds and common denominator, and computes the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4491a2-8adb-41ec-9cc0-0af6168349d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# CONTEXT #\n",
    "I want to generate Python code to showcase our product feature for\n",
    "serving open source large language models such as Code Llama series\n",
    "on Anyscale Endpoints. The product feature is Anyscale Endpoints, which \n",
    "serves all Llama series models and the Mistral series too. For this\n",
    "instance, we want to show case Code Llama 70B just released by Meta AI, and \n",
    "now hosted by Anyscale Endpoints\n",
    "\n",
    "#############\n",
    "\n",
    "# OBJECTIVE # \n",
    "Create a Python code that generates a list of fractions between 1 and 10.\n",
    "It then generates a common denominator, and adds the sum of all fractions.\n",
    "\n",
    "#############\n",
    "\n",
    "# STYLE #\n",
    "Use the Google style of formating Python code.\n",
    "\n",
    "#############\n",
    "\n",
    "# AUDIENCE #\n",
    "Your code should be well commented and aimed at both beginner and\n",
    "intermediate Python programers. \n",
    "\n",
    "#############\n",
    "\n",
    "# RESPONSE #\n",
    "Generate the entire code that can be easily copy and pasted into file: 'add_fractions.py'. Also,\n",
    "provide specific instructions how to compile the code, run it, and any \n",
    "additional python packages it needs. The final output of the code\n",
    "should be a list of fractions genetated, their sum, and their common denominator.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb76dc03-8f1b-4570-96b2-a65f949941fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAnswer - Python code\u001b[0m:\n",
      " 1. Install dependencies:\n",
      "    * We will use the `fractions` module in Python 3 to generate fractions. Run the following command to install the module:\n",
      "    ```\n",
      "    pip install fractions\n",
      "    ```\n",
      "2. Create a file named `add_fractions.py`.\n",
      "3. Copy and paste the following code:\n",
      "\n",
      "```python\n",
      "# add_fractions.py\n",
      "\n",
      "from fractions import Fraction\n",
      "\n",
      "# Create a list of fractions between 1 and 10\n",
      "fractions = [Fraction(i, 10) for i in range(1, 10)]\n",
      "\n",
      "# Calculate the sum of all fractions\n",
      "sum_fractions = sum(fractions, 0)\n",
      "\n",
      "# Calculate the common denominator\n",
      "common_denominator = sum_fractions.denominator\n",
      "\n",
      "# Print the result\n",
      "print(\"Fractions:\")\n",
      "for f in fractions:\n",
      "    print(f\" - {f}\")\n",
      "print()\n",
      "print(f\"Sum: {sum_fractions}\")\n",
      "print(f\"Common Denominator: {common_denominator}\")\n",
      "```\n",
      "\n",
      "4. Run the code using the command:\n",
      "\n",
      "```\n",
      "python add_fractions.py\n",
      "```\n",
      "\n",
      "Example output:\n",
      "\n",
      "```\n",
      "$ python add_fractions.py\n",
      "Fractions:\n",
      " - 1/10\n",
      " - 1/5\n",
      " - 3/10\n",
      " - 2/5\n",
      " - 1/2\n",
      " - 3/5\n",
      " - 7/10\n",
      " - 4/5\n",
      " - 9/10\n",
      "\n",
      "Sum: 59/50\n",
      "Common Denominator: 50\n",
      "```\n",
      "\n",
      "5. We can further simplify the code by removing the `fractions` list and calculating the sum directly as follows:\n",
      "\n",
      "```python\n",
      "# add_fractions_simplified.py\n",
      "\n",
      "from fractions import Fraction\n",
      "\n",
      "# Calculate the sum of all fractions\n",
      "sum_fractions = sum(Fraction(i, 10) for i in range(1, 10))\n",
      "\n",
      "# Calculate the common denominator\n",
      "common_denominator = sum_fractions.denominator\n",
      "\n",
      "# Print the result\n",
      "print(f\"Sum: {sum_fractions}\")\n",
      "print(f\"Common Denominator: {common_denominator}\")\n",
      "```\n",
      "\n",
      "6. Run the simplified code using the command:\n",
      "\n",
      "```\n",
      "python add_fractions_simplified.py\n",
      "```\n",
      "\n",
      "Example output:\n",
      "\n",
      "```\n",
      "$ python add_fractions_simplified.py\n",
      "Sum: 59/50\n",
      "Common Denominator: 50\n",
      "```\n",
      "\n",
      "7. Now, we can add some extra code to generate the Code Llama 70B model and serve it using Anyscale Endpoints. Here's a sample code with inline comments:\n",
      "\n",
      "```python\n",
      "# add_fractions_with_llama_70b.py\n",
      "\n",
      "# Import necessary libraries\n",
      "from fractions import Fraction\n",
      "from meta.pretrain.featurize.featurize import Featurizer  # This is a Meta AI library\n",
      "from anyscale.sdk import Model  # This is an Anyscale Endpoints library\n",
      "\n",
      "# Load the Code Llama 70B model\n",
      "llama_70b = Model.from_pretrained(\"code-llama-70B\")\n",
      "\n",
      "# Create a list of fractions between 1 and 10\n",
      "fractions = [Fraction(i, 10) for i in range(1, 10)]\n",
      "\n",
      "# Calculate the sum of all fractions\n",
      "sum_fractions = sum(fractions, 0)\n",
      "\n",
      "# Calculate the common denominator\n",
      "common_denominator = sum_fractions.denominator\n",
      "\n",
      "# Print the result\n",
      "print(\"Fractions:\")\n",
      "for f in fractions:\n",
      "    print(f\" - {f}\")\n",
      "print()\n",
      "print(f\"Sum: {sum_fractions}\")\n",
      "print(f\"Common Denominator: {common_denominator}\")\n",
      "\n",
      "\n",
      "#---------------Serve the model using Anyscale Endpoints----------------------\n",
      "\n",
      "# Initialize the Anyscale Endpoints model\n",
      "endpoint = Endpoint(\"Code Llama 70B\")\n",
      "\n",
      "# Specify the input and output format\n",
      "input_format = \"string\"\n",
      "output_format = \"string\"\n",
      "\n",
      "# Define the endpoint's predict function\n",
      "def predict(input_str):\n",
      "    # Convert input to tokens\n",
      "    input_tokens = Featurizer.featurize(input_str)\n",
      "    \n",
      "    # Run the model\n",
      "    prediction = llama_70b(input_tokens)\n",
      "    \n",
      "    # Convert output to string\n",
      "    output_str = str(prediction)\n",
      "    \n",
      "    return output_str\n",
      "\n",
      "# Deploy the model\n",
      "endpoint.deploy(predict, input_format, output_format)\n",
      "\n",
      "# Print the endpoint URL for accessing the model\n",
      "print(f\"Endpoint URL: {endpoint.url}\")\n",
      "```\n",
      "\n",
      "8. Run the code using the command:\n",
      "\n",
      "```\n",
      "python add_fractions_with_llama_70b.py\n",
      "```\n",
      "\n",
      "Example output:\n",
      "\n",
      "```\n",
      "$ python add_fractions_with_llama_70b.py\n",
      "Fractions:\n",
      " - 1/10\n",
      " - 1/5\n",
      " - 3/10\n",
      " - 2/5\n",
      " - 1/2\n",
      " - 3/5\n",
      " - 7/10\n",
      " - 4/5\n",
      " - 9/10\n",
      "\n",
      "Sum: 59/50\n",
      "Common Denominator: 50\n",
      "Endpoint URL: https://api.anyscale.com/<your_token>/Code-Llama-70B/v0/predict\n",
      "```\n",
      "\n",
      "9. Now, you can access the model hosted on Anyscale Endpoints by sending HTTP requests to the endpoint URL. The model takes a string as input and returns the result as a string.\n",
      "\n",
      "Please note that this is just a sample code and you may need to modify it according to your specific requirements.\n"
     ]
    }
   ],
   "source": [
    "response = get_commpletion(client, MODEL, system_content, user_prompt)\n",
    "print(f\"\\n{BOLD_BEGIN}Answer - Python code{BOLD_END}:\\n {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
