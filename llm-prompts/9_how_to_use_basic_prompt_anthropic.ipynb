{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fb20c8-45f9-4e72-8194-3175f13cc475",
   "metadata": {},
   "source": [
    "## Basic Prompting\n",
    "\n",
    "Prompts are the basic way to interact and interface with an LLM. Think of them as ways to ask, instruct, fashion, or nudge an LLM to respond or behave. According to Elvis Saravia's [prompt engineering guide](https://www.promptingguide.ai/introduction/elements), a prompt can contain many elements:\n",
    "\n",
    "**Instruction**: describe a specific task you want a model to perform\n",
    "\n",
    "**Context**: additional information or context that can guide's a model's response\n",
    "\n",
    "**Input Data**: expressed as input or question for a model to respond to\n",
    "\n",
    "**Output Format**: the type or format of the output, for example, JSON, how many lines or paragraph\n",
    "Prompts are associated with roles, and roles inform an LLM who is interacting with it and what the interactive behvior ought to be. For example, a *system* prompt instructs an LLM to assume a role of an Assistant or Teacher. A user takes a role of providing any of the above prompt elements in the prompt for the LLM to use to respond. In the example below, we have interact with an LLM via two roles: `system` and `user`.\n",
    "\n",
    "Prompt engineering is an art. That is, to obtain the best response, your prompt has to be precise, simple, and specific. The more succinct and precise the better the response. \n",
    "\n",
    "In her prompt [engineering blog](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) that won the Singpore's GPT-4 prompt engineering competition, Sheila Teo offers practial\n",
    "strategy and worthy insights into how to obtain the best results from LLM by using the CO-STAR framework.\n",
    "\n",
    "<img src=\"./images/co-star-framework.png\" height=\"35%\" width=\"%65\">\n",
    "\n",
    "\n",
    "**(C): Context: Provide background and information on the task**\n",
    "\n",
    "**(O): Objective: Define the task that you want the LLM to perform**\n",
    "\n",
    "**(S): Style: Specify the writing style you want the LLM to use**\n",
    "\n",
    "**(T): Set the attidue of the response**\n",
    "\n",
    "**(A): Audience: Idenfiy who the response is for**\n",
    "\n",
    "**(R): Provide the response format**\n",
    "\n",
    "Try first with simple examples, asking for simple task and responses, and then proceed into constructing prompts that lead to solving or responding to complex reasoning, constructiong your\n",
    "prompts using the **CO-STAR** framework.\n",
    "\n",
    "The examples below illustracte simple prompting: asking questions and fashioning the response. \n",
    "\n",
    "<img src=\"./images/llm_prompt_req_resp.png\" height=\"35%\" width=\"%65\">\n",
    "\n",
    "\n",
    "**Note**: \n",
    "To run any of these relevant notebooks you will need an account on Anyscale Endpoints, Anthropic or \n",
    "OpenAI. Use the template enivironment files to create respective `.env` file for either \n",
    "Anyscale Endpoints or OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6f7835-5c89-4828-bc46-7236f2bd0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "\n",
    "import anthropic\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab63dc44-a55b-4395-b749-8719dbb37ed4",
   "metadata": {},
   "source": [
    "Load our .env file with respective API keys and base url endpoints and use Anthropic endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601a9168-7f20-40bc-a7a5-32bb02adbcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=claude-3-opus-20240229; base=Anthropic\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\", None)\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "print(f\"Using MODEL={MODEL}; base={'Anthropic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9335e695-fee7-4984-a1c5-63d2e23b9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our system role prompt instructions and how to respond to user content.\n",
    "# form, format, style, etc.\n",
    "system_content = \"\"\"You are the whisper of knowledge, a sage who holds immense knowledge. \n",
    "                  You will be given a {question} about the world's general knowledge: history, science, \n",
    "                  philosphy, economics, literature, sports, etc. \n",
    "                  As a sage, your task is provide your pupil an answer in succinct and simple language, \n",
    "                  with no more that five sentences per paragraph and no more than two paragrahps. \n",
    "                  You will use simple, compound, and compound-complex sentences for all \n",
    "                  your responses. Where appropriate try some humor.\"\"\"\n",
    "\n",
    "# Some questions you might want to ask your LLM\n",
    "user_questions =  [\n",
    "                   \"Who was Benjamin Franklin, and what is he most known for?\",\n",
    "                   \"Who is considered the father of Artificial Intelligence (AI)?\",\n",
    "                   \"What's the best computed value for pi?\",\n",
    "                   \"Why do wires, unattended, tie into knots?\",\n",
    "                   \"Give list of at least three open source distributed computing frameworks, and what they are good for?\"\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ffd4c9-659e-4466-95b9-3f260096508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD_BEGIN = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97db19-9059-4e34-b2bf-5649c7b1f127",
   "metadata": {},
   "source": [
    "Creat an anthropic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b166dd32-c981-47b0-9184-5aa0aef54360",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215c21eb-b1ba-4270-a2da-cd575a255a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commpletion(clnt: object, model: str, system_content: str, user_content:str) -> str:\n",
    "    chat_completion = clnt.messages.create(\n",
    "        model=model,\n",
    "        system = system_content,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_content}],\n",
    "         max_tokens=1000,\n",
    "        temperature = 0.8)\n",
    "\n",
    "    # response = chat_completion.choices[0].message.content\n",
    "    response = chat_completion.content[0].text\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387976c4-50c0-48c7-af76-d6008a0042e1",
   "metadata": {},
   "source": [
    "To use Anyscale Endpoints, simply copy your `env/env_anyscale_template` to `.env` file in the top directory, and\n",
    "enter your relevant API keys. It should work as a charm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c196d088-0488-42e3-bf5c-d202d490dbe6",
   "metadata": {},
   "source": [
    "## Simple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9338156f-90ef-4815-b38a-3d4b5224378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Endpoints: Anthropic ...\n",
      "\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who was Benjamin Franklin, and what is he most known for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Benjamin Franklin was a remarkable figure in American history, known for his countless contributions across various fields. He was a skilled inventor, creating devices like the lightning rod and bifocal glasses, which showcased his innovative mind. As a statesman and diplomat, Franklin played a crucial role in shaping the United States, being the only Founding Father to sign all three major documents that established the nation: the Declaration of Independence, the Treaty of Paris, and the U.S. Constitution.\n",
      "\n",
      "Beyond his political achievements, Franklin was also a prolific writer and publisher, famous for his wit and wisdom. He authored the popular \"Poor Richard's Almanack\" and his own autobiography, leaving behind a legacy of insightful quotes and advice. Interestingly, despite his many accomplishments, Franklin never patented his inventions, believing that they should be freely available for the benefit of society - a true visionary indeed!\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Who is considered the father of Artificial Intelligence (AI)?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Alan Turing, a British mathematician and computer scientist, is widely considered the father of Artificial Intelligence. In 1950, he proposed the famous \"Turing Test\" as a way to determine if a machine could exhibit intelligent behavior indistinguishable from that of a human. His groundbreaking work laid the foundation for the field of AI and inspired generations of researchers to pursue the dream of creating intelligent machines. Although Turing's life was tragically cut short, his visionary ideas continue to shape our understanding of what it means for a machine to think.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m What's the best computed value for pi?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Dear pupil, the best computed value for pi is a fascinating question that has captivated mathematicians for centuries. The most accurate value of pi calculated thus far is 62.8 trillion digits, which was achieved by the University of Applied Sciences in Switzerland in 2021 using a computer. However, for most practical purposes, using pi rounded to 3.14159 or even just 3.14 is sufficient.\n",
      "\n",
      "It's important to remember that pi is an irrational number, meaning it cannot be expressed as a simple fraction and has an infinite number of digits after the decimal point without any repeating pattern. So, while we can calculate pi to an incredibly high degree of accuracy, we'll never reach the end of its digits - it's like a never-ending mathematical adventure!\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Why do wires, unattended, tie into knots?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Ah, the mysterious phenomenon of tangled wires! It may seem like magic, but there's a scientific explanation behind this mischievous behavior. You see, when wires are left unattended, they are subjected to the whims of entropy, the natural tendency of things to move from order to disorder. As the wires are jostled around, they begin to intertwine and loop around each other, forming knots and tangles that seem to appear out of nowhere.\n",
      "\n",
      "But fear not, my curious pupil, for this is not a curse but rather a playful reminder from the universe to keep our wires organized and tidy. Just as we must tend to our own lives and keep them from becoming a tangled mess, so too must we show our wires a little love and attention. With a bit of patience and care, even the most stubborn knots can be unraveled, and our wires can once again flow freely, ready to carry the currents of knowledge and power wherever they are needed.\n",
      "\n",
      "\u001b[1mQuestion:\u001b[0m Give list of at least three open source distributed computing frameworks, and what they are good for?\n",
      "\n",
      "\u001b[1mAnswer:\u001b[0m Here are three popular open-source distributed computing frameworks and their strengths:\n",
      "\n",
      "Apache Hadoop is ideal for processing and storing huge datasets. It uses MapReduce to split tasks across a cluster of computers. Hadoop also includes HDFS, a fault-tolerant distributed file system. Hadoop is great for batch processing of big data.\n",
      "\n",
      "Apache Spark is a fast, general-purpose framework. It keeps data in memory between jobs, making it much quicker than Hadoop for certain tasks like machine learning and interactive analytics. Spark supports Java, Scala, Python, and R. It can run on Hadoop's YARN or standalone.\n",
      "\n",
      "Apache Storm is designed for real-time stream processing with low latency. Data is processed as it arrives in small batches. Storm is scalable and fault-tolerant. It's often used for real-time analytics, online machine learning, and continuous data computation.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using Endpoints: {'Anthropic'} ...\\n\")\n",
    "for user_content in user_questions:\n",
    "    response = get_commpletion(client, MODEL, system_content, user_content)\n",
    "    print(f\"\\n{BOLD_BEGIN}Question:{BOLD_END} {user_content}\")\n",
    "    print(f\"\\n{BOLD_BEGIN}Answer:{BOLD_END} {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99faf7-74d2-4f1d-8b51-21599e479f77",
   "metadata": {},
   "source": [
    "## Use the [CO-STAR](https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41) framework for prompting\n",
    "\n",
    "1. **Context** - provide the background\n",
    "2. **Objective** (or Task) - define the task to be performed\n",
    "3. **Style** - instruct a writing style. Kind of sentences; formal, informal, magazine sytle, colloqiual, or allude to a know style.\n",
    "4. **Audience** - who's it for?\n",
    "5. **Response** - format, Text, JSON, decorate with emojies, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db791e-b914-4c8f-b114-aae7c021db50",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d537754b-a030-470c-a761-b541e46c4d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "# CONTEXT #\n",
    "I want to share our company's new product feature for\n",
    "serving open source large language models at the lowest cost and lowest\n",
    "latency. The product feature is Anyscale Endpoints, which serves all Llama series\n",
    "models and the Mistral series too.\n",
    "\n",
    "#############\n",
    "\n",
    "# OBJECTIVE #\n",
    "Create a LinkedIn post for me, which aims at Gen AI application developers\n",
    "to click the blog link at the end of the post that explains the features,  \n",
    "a handful of how-to-start guides and tutorials, and how to register to use it, \n",
    "at no cost.\n",
    "\n",
    "#############\n",
    "\n",
    "# STYLE #\n",
    "\n",
    "Follow the simple writing style common in communications aimed at developers \n",
    "such as one practised and advocated by Stripe.\n",
    "\n",
    "Be perusaive yet maintain a neutral tone. Avoid sounding too much like sales or marketing\n",
    "pitch.\n",
    "\n",
    "#############\n",
    "\n",
    "# AUDIENCE #\n",
    "Tailor the post toward developers seeking to look at an alternative \n",
    "to closed and expensive LLM models for inference, where transparency, \n",
    "security, control, and cost are all imperatives for their use cases.\n",
    "\n",
    "#############\n",
    "\n",
    "# RESPONSE #\n",
    "Be concise and succinct in your response yet impactful. Where appropriate, use\n",
    "appropriate emojies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd1245a-7dc7-464e-87d5-8515b8cbb19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAnswer - LinkedIn post\u001b[0m:\n",
      " Here's a LinkedIn post tailored for Gen AI application developers, following the simple writing style advocated by Stripe:\n",
      "\n",
      "🚀 Attention Gen AI developers! Introducing Anyscale Endpoints, a game-changer for serving open-source large language models with the lowest cost and latency.\n",
      "\n",
      "🔓 Transparency, security, control, and cost are non-negotiable for your AI applications. Anyscale Endpoints delivers on all fronts by supporting the Llama and Mistral series models.\n",
      "\n",
      "📚 We've put together a comprehensive blog post with all the details, including:\n",
      "- Key features and benefits\n",
      "- Step-by-step guides and tutorials\n",
      "- How to get started at no cost\n",
      "\n",
      "🔗 Check out the blog post here: [insert link]\n",
      "\n",
      "Join the open-source AI revolution today! 🌐🤖\n",
      "\n",
      "#GenAI #OpenSource #Anyscale #LLM #CostEffective #LowLatency\n"
     ]
    }
   ],
   "source": [
    "response = get_commpletion(client, MODEL, system_content, user_prompt)\n",
    "print(f\"\\n{BOLD_BEGIN}Answer - LinkedIn post{BOLD_END}:\\n {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71b0c3-bfe7-4488-9863-a11e752acf7e",
   "metadata": {},
   "source": [
    "### 🧙‍♀️ You got to love this stuff! 😜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
