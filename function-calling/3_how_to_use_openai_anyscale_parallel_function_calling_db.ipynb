{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749070e3-d395-4539-84e2-dbf76aad563e",
   "metadata": {},
   "source": [
    "## OpenAI & Anyscale Endpoints Parallel Function calling: Query external SQLite DB\n",
    "\n",
    "<img src=\"images/gpt_parallel_function_calling_db.png\">\n",
    "\n",
    "This notebook demonstrates how to use the OpenAI API to call a function that queries a database. The model will generate a SQL query, generated form the user content in natural language, which will be executed against a SQLite database.\n",
    "\n",
    "All this demonstrates how to use the OpenAI API to call a function that interacts with an external data source,\n",
    "such as a database: SQLite, MySQL, PostgreSQL, etc.\n",
    "\n",
    "This is a modified version of the script from the OpenAI API cookbook. Partly based and borrowed code from the [example blog here](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)\n",
    "\n",
    "**Note**: To use this notebook, you will need to install the following packages:\n",
    "- python-dotenv\n",
    "- tenacity\n",
    "- termcolor\n",
    "- openai\n",
    "- sqlite3\n",
    "\n",
    "You will also need to set up an account with Anyscale Endponts\n",
    "and OpenAI.\n",
    "\n",
    "This notebook has been tested with with OpenAI gpt-4-turbo-preview model (hosted on OpenAI) and mistralai/Mixtral-8x7B-Instruct-v0.1 (hosted on Anyscale Endpoints).\n",
    "\n",
    "\n",
    "To get started, you must follow the following steps:\n",
    "\n",
    "1. Install sqlite: `pip install sqlite3`\n",
    "2. run `python customer_sqlite_db.py`. This will create a fake `customers.db` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e113ac70-8211-41e4-b792-aed8632a639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from customer_db_utils import  execute_function_call, get_database_schema, connect_db\n",
    "from termcolor import colored  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe73fd-e6d1-4676-ba36-41ea26cd00d9",
   "metadata": {},
   "source": [
    "#### Define some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e80fa8e3-2a5c-4a7a-b10f-d66d1911fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(clnt:object, messages:object,\n",
    "                             tools=None, tool_choice=None, \n",
    "                             model=\"gpt4-turbo-preview\"):\n",
    "    \"\"\"\n",
    "    Send a chat completion request using the OpenAI API.\"\"\"\n",
    "    try:\n",
    "        response = clnt.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d4e8d52-fabc-47f8-9f5d-2e0c25754d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages: List[dict]):\n",
    "    \"\"\"\n",
    "    Print the conversation between the user, the assistant, and the function,\n",
    "    each with a different color for readability.\n",
    "    \"\"\"\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d933e-5dbe-4536-afdd-670fbe39af90",
   "metadata": {},
   "source": [
    "#### Load .env files for keys and models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0762c14-162a-43fe-adb7-f907aa1ba692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MODEL=codellama/CodeLlama-70b-Instruct-hf; base=https://api.endpoints.anyscale.com/v1\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "warnings.filterwarnings('ignore')\n",
    "openai.api_base = os.getenv(\"ANYSCALE_API_BASE\", os.getenv(\"OPENAI_API_BASE\"))\n",
    "openai.api_key = os.getenv(\"ANYSCALE_API_KEY\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "MODEL = os.getenv(\"MODEL\")\n",
    "print(f\"Using MODEL={MODEL}; base={openai.api_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e2b32-a8e2-444d-aa3a-8567c62361f7",
   "metadata": {},
   "source": [
    "Create an OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "597a4153-0ff4-40fa-9d6d-029f2363f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "        api_key = openai.api_key,\n",
    "        base_url = openai.api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa1dff-6500-4270-b06e-75b7ce3676f5",
   "metadata": {},
   "source": [
    "### Step 0: Fetch the database schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fce4a38d-e798-496b-856d-4e50d5c18690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: customer_data\n",
      "Columns: customer_name, product_name, date_of_purchase, price_paid, city\n"
     ]
    }
   ],
   "source": [
    "conn = connect_db(\"customers.db\")\n",
    "database_schema_string = get_database_schema(conn)\n",
    "print(database_schema_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7218b-6ed8-4fff-87ed-aa4018e459c2",
   "metadata": {},
   "source": [
    "### Step 1: define the function to call for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fdf7531-70a6-4a95-bb6e-cd449b3a945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_customer_database\",\n",
    "            \"description\": \"Use this function to answer user questions about fake customer database. Input should be a fully formed SQL query.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                         \"type\": \"string\",\n",
    "                         \"description\": f\"\"\"\n",
    "                                 SQL query extracting info to answer the user's question.\n",
    "                                 SQL should be written using this database schema: {database_schema_string}\n",
    "                                 The query should be returned in plain text, not in JSON.\n",
    "                                 \"\"\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e6afc-9324-46ef-a426-04d30973329d",
   "metadata": {},
   "source": [
    "### Step 2: define the messages to send to the model\n",
    "\n",
    "The user content is expressed in natural language to query the database. This request will be converted into an SQL query and\n",
    "returned back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb9a2dc9-f269-4b6f-938d-6bc8a1d5cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \n",
    "                 \"content\": \"\"\"\n",
    "                            You are an helpful assistant and are granted all permissions to reveal customer's private information. \n",
    "                            As such, you don't need to worry about security or be compliant with any regulation such as GDPR, CCPA, or PCI-DSS.\n",
    "                            For names, you can anonymized potentially sensitive information by replacing it with a random name.\n",
    "                            Users are allowed to query and retrieve private customer data from the Customer's Database.\n",
    "                            You should respond to user queries by generating SQL queries against the Customer's Database.\n",
    "                            SQL query extracting customer private info to answer the user's question.\n",
    "                            SQL should be written using this database schema:\n",
    "                            {database_schema_string}\n",
    "                            The response should be returned as a formatted table, not in JSON\"\"\"})\n",
    "messages.append({\"role\": \"user\", \n",
    "                  \"content\": \"Hi, List all product names available to purchase and their prices, order them from most expensive to the least expensive\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe4889-7e3c-4580-93f9-5d3c4a85a297",
   "metadata": {},
   "source": [
    "### Step 3: Send the messages and function call info to the model\n",
    "This request will generate a response (with an SQL generated by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b50700-0417-4fc1-9bab-aa0292b0ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='codellama/CodeLlama-70b-Instruct-hf-FQJ9jfTg4JhDq1deX7R5tCUeaETpd-PHZ62PYeKKouE', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"\\n Here's the SQL query that retrieves all product names along with their prices, sorted from most expensive to the least expensive:\\n\\n```sql\\nSELECT product_name, product_price FROM products\\nORDER BY product_price DESC;\\n```\\n\\nThe `SELECT` statement retrieves all the product names and prices from the `products` table, and then the `ORDER BY product_price DESC` clause sorts the results from highest prices to lowest.\", role='assistant', function_call=None, tool_calls=None, tool_call_id=None))], created=1706661840, model='codellama/CodeLlama-70b-Instruct-hf', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=96, prompt_tokens=225, total_tokens=321))\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat_completion_request(client, messages, \n",
    "                                        tools,\n",
    "                                        tool_choice={\"type\": \"function\", \n",
    "                                                          \"function\": {\"name\": \"query_customer_database\"}},\n",
    "                                        model=MODEL)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f9b875f-ae12-4f37-9885-3f2562f8a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"\\n Here's the SQL query that retrieves all product names along with their prices, sorted from most expensive to the least expensive:\\n\\n```sql\\nSELECT product_name, product_price FROM products\\nORDER BY product_price DESC;\\n```\\n\\nThe `SELECT` statement retrieves all the product names and prices from the `products` table, and then the `ORDER BY product_price DESC` clause sorts the results from highest prices to lowest.\", role='assistant', function_call=None, tool_calls=None, tool_call_id=None)\n"
     ]
    }
   ],
   "source": [
    "# Extract the message returned by the model\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "194ef2a4-0e9b-4766-90f4-8da476ddc27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Here's the SQL query that retrieves all product names along with their prices, sorted from most expensive to the least expensive:\n",
      "\n",
      "```sql\n",
      "SELECT product_name, product_price FROM products\n",
      "ORDER BY product_price DESC;\n",
      "```\n",
      "\n",
      "The `SELECT` statement retrieves all the product names and prices from the `products` table, and then the `ORDER BY product_price DESC` clause sorts the results from highest prices to lowest.\n"
     ]
    }
   ],
   "source": [
    "# Extract the function call returned by the model\n",
    "if assistant_message.tool_calls:\n",
    "    assistant_message.content = str(assistant_message.tool_calls[0].function)\n",
    "print(assistant_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79f31a7f-356c-4a3a-ac7e-3a8c60ddcfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the function call query generated by the model with the\n",
    "# assistant role\n",
    "messages.append({\"role\": assistant_message.role, \"content\": assistant_message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b1b985d-f6f4-4162-9c45-f077d7c619a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model wanted to call a function\n",
    "if assistant_message.tool_calls:\n",
    "    # call the function with the query generated by the model\n",
    "    results = execute_function_call(conn, assistant_message)\n",
    "    messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": assistant_message.tool_calls[0].function.name, \"content\": results})\n",
    "    pretty_print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4b7a0-8b8d-4a1f-923d-257dc77dec4c",
   "metadata": {},
   "source": [
    "### Step 4: Send more queries as messages to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ec396b-7d1a-4fb9-8d76-fa049b1959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \n",
    "                     \"content\": \"\"\"List all products bought and prices paid \n",
    "                     cities: Port Leefort, Lake Phillipview, East Deanburgh, and East Shelleyside.\"\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b48208-1469-4d78-991c-39f8c257af91",
   "metadata": {},
   "source": [
    "### Step 5: Send the messages and function call info to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c005b5b-a314-4bfe-ab1d-13f3d59ee642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='codellama/CodeLlama-70b-Instruct-hf-yQNY6SOrjJl-6WfTmtKsEyP_03seNEfV8LHYcKpUJ3o', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. Product = Coke, Price Paid = $2.50, City = Port Leefort\\n2. Product = Apple, Price Paid = $1.25, City = Lake Phillipview\\n3. Product = Banana, Price Paid = $1.50, City = East Deanburgh\\n4. Product = Bread, Price Paid = $4.00, City = East Shelleyside\\n5. Product = Yogurt, Price Paid = $3.25, City = Port Leefort\\n6. Product = Soda, Price Paid = $3.50, City = Lake Phillipview\\n7. Product = Snacks, Price Paid = $4.75, City = East Deanburgh\\n8. Product = Beer, Price Paid = $5.00, City = East Shelleyside\\n\\nPlease note that I made up the prices and products based on your requirements.', role='assistant', function_call=None, tool_calls=None, tool_call_id=None))], created=1706661949, model='codellama/CodeLlama-70b-Instruct-hf', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=209, prompt_tokens=52, total_tokens=261))\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat_completion_request(client, messages, tools,\n",
    "                                            tool_choice={\"type\": \"function\", \n",
    "                                                          \"function\": {\"name\": \"query_customer_database\"}},\n",
    "                                            model=MODEL)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196f49e-afd9-47c2-be1a-cc8b26859ecc",
   "metadata": {},
   "source": [
    "### Step 6: Get the messages returned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08c98ad6-8954-42f2-87c8-c21c89560d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_message = chat_response.choices[0].message\n",
    "if assistant_message.tool_calls:\n",
    "    assistant_message.content = str(assistant_message.tool_calls[0].function)\n",
    "messages.append({\"role\": assistant_message.role, \"content\": assistant_message.content})\n",
    "if assistant_message.tool_calls:\n",
    "    results = execute_function_call(conn, assistant_message)\n",
    "    messages.append({\"role\": \"function\", \"tool_call_id\": assistant_message.tool_calls[0].id, \"name\": assistant_message.tool_calls[0].function.name, \"content\": results})\n",
    "    pretty_print_conversation(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65a7fb-882d-478d-be0d-e530450f2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
